{
  "permissions": {
    "allow": [
      "Bash(git add:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: configure pyproject.toml for pip distribution\n\n- Add complete package metadata \\(authors, keywords, classifiers\\)\n- Organize optional dependencies by feature \\(postgres, bigquery, s3, mongodb, snowflake\\)\n- Add airflow extra for Airflow integration\n- Add CLI entry point \\(dataforge command\\)\n- Move FastAPI/uvicorn to optional ''api'' extra\n- Add development tools \\(pytest-cov, pytest-mock, mypy, pre-commit\\)\n- Include project URLs \\(homepage, docs, repository, issues\\)\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add robust CLI with click and rich\n\nCommands implemented:\n- dataforge run <pipeline>: Execute pipelines with progress feedback\n- dataforge list: Display all pipelines in a formatted table\n- dataforge validate: Validate pipeline configuration\n- dataforge init <name>: Create new pipeline template\n- dataforge test <pipeline>: Test connector connections\n\nFeatures:\n- Rich console output with colors and tables\n- Progress bars with spinners\n- Error handling with detailed messages\n- Verbose mode for debugging\n- Configuration file support\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add Airflow integration with operators, hooks and sensors\n\nAirflow Components:\n- DataForgeOperator: Execute pipelines as Airflow tasks\n- DataForgeValidateOperator: Validate pipeline configs\n- DataForgeHook: Core hook for pipeline interaction\n- DataForgeSensor: Wait for pipeline config availability\n- DataForgeConnectionSensor: Wait for config file\n\nFeatures:\n- XCom support for passing results between tasks\n- Template fields for dynamic configuration\n- Validation before execution\n- Proper error handling and logging\n- UI colors for Airflow web interface\n\nExample DAGs:\n- Single pipeline execution with validation\n- Parallel pipeline execution\n- Sensor-based coordination\n\nInstall with: pip install data-forge[airflow]\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add advanced ETL features \\(retry, rate limiting, caching, validation\\)\n\nRetry Logic:\n- RetryConfig for configurable retry behavior\n- Exponential backoff with tenacity integration\n- @with_retry decorator for async functions\n- CircuitBreaker pattern to prevent cascading failures\n\nRate Limiting:\n- RateLimiter: Token bucket algorithm\n- SlidingWindowRateLimiter: More accurate rate limiting\n- AdaptiveRateLimiter: Auto-adjusts based on response codes\n- Async context manager support\n\nCaching:\n- MemoryCache: In-memory LRU cache with TTL\n- DiskCache: Persistent disk-based cache\n- RequestCache: High-level HTTP request caching\n- Configurable backends \\(memory/disk\\)\n\nValidation:\n- SchemaValidator: Pydantic-based schema validation\n- DataQualityValidator: Custom quality rules \\(min/max, regex, enum\\)\n- Support for single items and batch validation\n\nAll modules include:\n- Structured logging with structlog\n- Async-first design\n- Comprehensive error handling\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "WebFetch(domain:alexwohlbruck.github.io)",
      "Bash(find:*)",
      "Bash(MANIFEST.in << 'EOF'\ninclude README.md\ninclude LICENSE\ninclude CHANGELOG.md\ninclude CONTRIBUTING.md\ninclude logo_forgeflow.svg\nrecursive-include forgeflow *.py\nrecursive-include config *.yaml\nrecursive-exclude * __pycache__\nrecursive-exclude * *.py[co]\nEOF)",
      "Bash(.pypirc.example << 'EOF'\n[pypi]\nusername = __token__\npassword = pypi-AgEIcHlwaS5vcmc...\n\n[testpypi]\nusername = __token__\npassword = pypi-AgENdGVzdC5weXBpLm9yZw...\nEOF)",
      "Bash(pip install:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nRename project from data-forge to forgeflow\n\n- Rename package directory from dataforge/ to forgeflow/\n- Update all imports and references across codebase\n- Update documentation \\(README, CHANGELOG, CONTRIBUTING\\)\n- Update pyproject.toml configuration\n- Update author email to luiz.dataeng@gmail.com\n- Add MANIFEST.in for PyPI distribution\n- Prepare for v0.1.0 release\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git tag:*)",
      "Bash(python -m build)",
      "Bash(python -m twine check:*)",
      "Bash(git commit:*)",
      "Bash(chmod +x:*)",
      "Bash(docs/index.md << 'EOF'\n# ForgeFlow Documentation\n\nWelcome to **ForgeFlow** - A modern ETL framework for API ingestion, data transformation, and distribution.\n\n## ðŸŽ¯ What is ForgeFlow?\n\nForgeFlow is a Python framework designed to simplify the process of:\n- Fetching data from REST APIs\n- Transforming and normalizing JSON data\n- Loading data into various sinks \\(PostgreSQL, DuckDB, files\\)\n- Orchestrating data pipelines with Apache Airflow\n\nBuilt with async-first architecture using Python 3.11+, ForgeFlow provides a declarative YAML-based configuration system for building robust data pipelines.\n\n## ðŸ“š Documentation Structure\n\n- **[Getting Started]\\(guides/getting-started.md\\)** - Installation and quick start\n- **[User Guide]\\(guides/user-guide.md\\)** - Comprehensive usage guide\n- **[Configuration]\\(guides/configuration.md\\)** - YAML configuration reference\n- **[API Reference]\\(api/index.md\\)** - Detailed API documentation\n- **[Examples]\\(examples/index.md\\)** - Real-world examples and tutorials\n- **[Contributing]\\(../CONTRIBUTING.md\\)** - How to contribute\n\n## âš¡ Quick Start\n\n### Installation\n\n```bash\npip install forgeflow\n```\n\n### Basic Usage\n\nCreate a `config/pipelines.yaml`:\n\n```yaml\npipelines:\n  - name: my_first_pipeline\n    enabled: true\n    connector:\n      type: http\n      config:\n        url: https://api.example.com/data\n    transformer:\n      type: json_normalizer\n      config:\n        flatten: true\n    sinks:\n      - type: duckdb\n        config:\n          database: data/output.duckdb\n          table: api_data\n```\n\nRun it:\n\n```bash\nforgeflow run my_first_pipeline\n```\n\n## ðŸš€ Features\n\n- âœ… **Async-first** - Built on asyncio for high performance\n- âœ… **Type-safe** - Full type hints with Pydantic models\n- âœ… **Extensible** - Easy to add custom connectors, transformers, and sinks\n- âœ… **Airflow Integration** - Native support for Apache Airflow\n- âœ… **Rich CLI** - Beautiful command-line interface\n- âœ… **Multiple Sinks** - PostgreSQL, DuckDB, Files \\(JSON, Parquet\\)\n\n## ðŸ“– Learn More\n\nCheck out our comprehensive guides:\n\n1. [Installation Guide]\\(guides/getting-started.md\\)\n2. [Core Concepts]\\(guides/core-concepts.md\\)\n3. [Pipeline Configuration]\\(guides/configuration.md\\)\n4. [Extending ForgeFlow]\\(guides/extending.md\\)\n\n## ðŸ¤ Community\n\n- **GitHub**: [leduardoaraujo/forgeflow]\\(https://github.com/leduardoaraujo/forgeflow\\)\n- **Issues**: [Report bugs or request features]\\(https://github.com/leduardoaraujo/forgeflow/issues\\)\n\n## ðŸ“„ License\n\nForgeFlow is released under the MIT License. See [LICENSE]\\(../LICENSE\\) for details.\nEOF)",
      "Bash(docs/guides/getting-started.md << 'EOF'\n# Getting Started with ForgeFlow\n\nThis guide will help you get started with ForgeFlow in just a few minutes.\n\n## Installation\n\n### Requirements\n\n- Python 3.11 or higher\n- pip or uv package manager\n\n### Basic Installation\n\n```bash\npip install forgeflow\n```\n\n### Installation with Optional Dependencies\n\nForgeFlow supports multiple sinks and integrations. Install what you need:\n\n```bash\n# PostgreSQL support\npip install forgeflow[postgres]\n\n# DuckDB support\npip install forgeflow[duckdb]\n\n# API server\npip install forgeflow[api]\n\n# Airflow integration\npip install forgeflow[airflow]\n\n# Everything\npip install forgeflow[all]\n```\n\n## Your First Pipeline\n\n### 1. Create Configuration Directory\n\n```bash\nmkdir -p config\n```\n\n### 2. Create Pipeline Configuration\n\nCreate `config/pipelines.yaml`:\n\n```yaml\npipelines:\n  - name: hello_forgeflow\n    enabled: true\n    connector:\n      type: http\n      config:\n        url: https://jsonplaceholder.typicode.com/users\n        method: GET\n        timeout: 30\n    transformer:\n      type: json_normalizer\n      config:\n        flatten: true\n    sinks:\n      - type: file\n        config:\n          path: data/output\n          format: json\n```\n\n### 3. Run Your Pipeline\n\n```bash\nforgeflow run hello_forgeflow\n```\n\n### 4. View Results\n\n```bash\ncat data/output/hello_forgeflow.json\n```\n\n## CLI Commands\n\nForgeFlow provides several CLI commands:\n\n```bash\n# List all pipelines\nforgeflow list\n\n# Run a specific pipeline\nforgeflow run <pipeline-name>\n\n# Validate configuration\nforgeflow validate\n\n# Create new pipeline template\nforgeflow init <pipeline-name>\n\n# Test pipeline connection\nforgeflow test <pipeline-name>\n```\n\n## Next Steps\n\nNow that you have ForgeFlow running, explore:\n\n- [Core Concepts]\\(core-concepts.md\\) - Understand ForgeFlow architecture\n- [Configuration Guide]\\(configuration.md\\) - Deep dive into YAML configuration\n- [Examples]\\(../examples/index.md\\) - Real-world use cases\n- [Extending ForgeFlow]\\(extending.md\\) - Build custom components\n\n## Troubleshooting\n\n### Command not found: forgeflow\n\nMake sure your Python scripts directory is in PATH:\n\n```bash\n# Linux/Mac\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n# Windows\n# Add %USERPROFILE%\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python3XX\\\\Scripts to PATH\n```\n\n### Import errors\n\nMake sure you installed ForgeFlow in your active virtual environment:\n\n```bash\npip list | grep forgeflow\n```\n\n### Need Help?\n\n- Check the [User Guide]\\(user-guide.md\\)\n- Search [GitHub Issues]\\(https://github.com/leduardoaraujo/forgeflow/issues\\)\n- Create a new issue if you found a bug\nEOF)",
      "Bash(docs/guides/core-concepts.md << 'EOF'\n# Core Concepts\n\nUnderstanding ForgeFlow's architecture and core concepts will help you build better data pipelines.\n\n## Architecture Overview\n\nForgeFlow follows a simple, composable architecture:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Connector  â”‚ â”€â”€> Fetch data from source\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Transformer â”‚ â”€â”€> Transform and normalize data\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Sink     â”‚ â”€â”€> Load data to destination\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Components\n\n### 1. Connectors\n\nConnectors are responsible for **fetching data** from external sources.\n\n**Built-in Connectors:**\n- `http` - Generic HTTP requests\n- `rest` - RESTful API client with base URL patterns\n\n**Example:**\n```yaml\nconnector:\n  type: http\n  config:\n    url: https://api.example.com/data\n    method: GET\n    headers:\n      Authorization: Bearer ${API_TOKEN}\n```\n\n### 2. Transformers\n\nTransformers **process and normalize** the fetched data.\n\n**Built-in Transformers:**\n- `json_normalizer` - Flatten nested JSON structures\n\n**Example:**\n```yaml\ntransformer:\n  type: json_normalizer\n  config:\n    flatten: true\n    separator: \"_\"\n```\n\n### 3. Sinks\n\nSinks **write data** to destinations.\n\n**Built-in Sinks:**\n- `postgres` - PostgreSQL database\n- `duckdb` - DuckDB analytical database\n- `file` - Local files \\(JSON, JSONL, Parquet\\)\n\n**Example:**\n```yaml\nsinks:\n  - type: duckdb\n    config:\n      database: data/warehouse.duckdb\n      table: users\n      mode: append\n```\n\n## Pipeline Lifecycle\n\n1. **Configuration Loading** - Parse YAML and validate\n2. **Connector Initialization** - Setup connection to source\n3. **Data Fetching** - Retrieve data asynchronously\n4. **Transformation** - Process data through transformer\n5. **Writing** - Load data to all configured sinks\n6. **Cleanup** - Close connections and cleanup resources\n\n## Async-First Design\n\nForgeFlow is built with `asyncio` for better performance:\n\n```python\nimport asyncio\nfrom forgeflow.pipeline.executor import PipelineExecutor\nfrom forgeflow.pipeline.loader import PipelineLoader\n\nasync def main\\(\\):\n    pipelines = PipelineLoader.load_from_file\\('config/pipelines.yaml'\\)\n    executor = PipelineExecutor\\(\\)\n    \n    for pipeline in pipelines:\n        await executor.execute\\(pipeline\\)\n\nasyncio.run\\(main\\(\\)\\)\n```\n\n## Configuration as Code\n\nPipelines are defined in YAML for:\n- **Version Control** - Track changes in git\n- **Declarative** - What, not how\n- **Readable** - Non-programmers can understand\n- **Portable** - Easy to share and deploy\n\n## Error Handling\n\nForgeFlow includes built-in utilities:\n\n- **Retry Logic** - Automatic retries with exponential backoff\n- **Rate Limiting** - Respect API rate limits\n- **Validation** - Schema validation with Pydantic\n- **Structured Logging** - Debug-friendly logs\n\n## Next Steps\n\n- [Configuration Reference]\\(configuration.md\\)\n- [Building Custom Components]\\(extending.md\\)\n- [Airflow Integration]\\(airflow.md\\)\nEOF)",
      "Bash(docs/mkdocs.yml << 'EOF'\nsite_name: ForgeFlow Documentation\nsite_description: Modern ETL framework for API ingestion and data distribution\nsite_author: Luiz Araujo\nsite_url: https://forgeflow.readthedocs.io\n\nrepo_name: leduardoaraujo/forgeflow\nrepo_url: https://github.com/leduardoaraujo/forgeflow\nedit_uri: edit/main/docs/\n\ntheme:\n  name: material\n  palette:\n    - scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n    - scheme: slate\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.expand\n    - navigation.top\n    - search.suggest\n    - search.highlight\n    - content.code.copy\n    - content.code.annotate\n\nmarkdown_extensions:\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.tabbed:\n      alternate_style: true\n  - tables\n  - attr_list\n  - md_in_html\n\nnav:\n  - Home: index.md\n  - Getting Started:\n    - Installation: guides/getting-started.md\n    - Core Concepts: guides/core-concepts.md\n  - User Guide:\n    - Configuration: guides/configuration.md\n    - Extending ForgeFlow: guides/extending.md\n  - Examples:\n    - Overview: examples/index.md\n  - API Reference:\n    - Overview: api/index.md\n  - Contributing: ../CONTRIBUTING.md\n\nplugins:\n  - search\n  - autorefs\n\nextra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/leduardoaraujo/forgeflow\n    - icon: fontawesome/brands/python\n      link: https://pypi.org/project/forgeflow/\nEOF)",
      "Bash(docs/README.md << 'EOF'\n# ForgeFlow Documentation\n\nThis directory contains the documentation for ForgeFlow.\n\n## Building Documentation Locally\n\n### Install MkDocs\n\n```bash\npip install mkdocs-material mkdocs-autorefs\n```\n\n### Serve Documentation\n\n```bash\ncd docs\nmkdocs serve\n```\n\nThen open http://127.0.0.1:8000 in your browser.\n\n### Build Documentation\n\n```bash\ncd docs\nmkdocs build\n```\n\nStatic site will be generated in `site/` directory.\n\n## Documentation Structure\n\n```\ndocs/\nâ”œâ”€â”€ index.md              # Homepage\nâ”œâ”€â”€ guides/               # User guides\nâ”‚   â”œâ”€â”€ getting-started.md\nâ”‚   â”œâ”€â”€ core-concepts.md\nâ”‚   â”œâ”€â”€ configuration.md\nâ”‚   â””â”€â”€ extending.md\nâ”œâ”€â”€ examples/             # Examples and tutorials\nâ”‚   â””â”€â”€ index.md\nâ”œâ”€â”€ api/                  # API reference\nâ”‚   â””â”€â”€ index.md\nâ””â”€â”€ assets/               # Images, diagrams, etc.\n```\n\n## Contributing to Documentation\n\n1. Edit Markdown files in `docs/`\n2. Test locally with `mkdocs serve`\n3. Submit a PR with your changes\n\nSee [CONTRIBUTING.md]\\(../CONTRIBUTING.md\\) for more details.\nEOF)",
      "Bash(docs/examples/index.md << 'EOF'\n# Examples\n\nReal-world examples and tutorials for ForgeFlow.\n\n## Basic Examples\n\n### Example 1: Fetch JSON from API\n\nFetch user data from JSONPlaceholder API and save to file:\n\n```yaml\npipelines:\n  - name: fetch_users\n    enabled: true\n    connector:\n      type: http\n      config:\n        url: https://jsonplaceholder.typicode.com/users\n        method: GET\n    transformer:\n      type: json_normalizer\n      config:\n        flatten: false\n    sinks:\n      - type: file\n        config:\n          path: data/users\n          format: json\n```\n\n### Example 2: Load to DuckDB\n\nFetch and load data into DuckDB for analytics:\n\n```yaml\npipelines:\n  - name: api_to_duckdb\n    enabled: true\n    connector:\n      type: http\n      config:\n        url: https://api.example.com/sales\n        headers:\n          Authorization: Bearer ${API_TOKEN}\n    transformer:\n      type: json_normalizer\n      config:\n        flatten: true\n    sinks:\n      - type: duckdb\n        config:\n          database: analytics.duckdb\n          table: sales\n          mode: replace\n```\n\n### Example 3: Multiple Sinks\n\nWrite to multiple destinations simultaneously:\n\n```yaml\npipelines:\n  - name: multi_sink_pipeline\n    enabled: true\n    connector:\n      type: rest\n      config:\n        base_url: https://api.example.com\n        endpoint: /data\n    transformer:\n      type: json_normalizer\n      config:\n        flatten: true\n    sinks:\n      - type: postgres\n        config:\n          host: localhost\n          database: warehouse\n          table: raw_data\n      - type: file\n        config:\n          path: backup/data\n          format: parquet\n      - type: duckdb\n        config:\n          database: analytics.duckdb\n          table: processed_data\n```\n\n## Advanced Examples\n\n### Airflow Integration\n\nSee [airflow_dag_example.py]\\(../../examples/airflow_dag_example.py\\) for a complete example.\n\n### Custom Connector\n\n```python\nfrom forgeflow.core.connector import BaseConnector\n\nclass MyCustomConnector\\(BaseConnector\\):\n    def validate_config\\(self\\) -> None:\n        required = [\"api_key\"]\n        missing = [k for k in required if k not in self.config]\n        if missing:\n            raise ValueError\\(f\"Missing: {missing}\"\\)\n    \n    async def fetch\\(self\\) -> dict:\n        # Your implementation\n        return {\"data\": []}\n    \n    async def close\\(self\\) -> None:\n        pass\n```\n\n## Tutorials\n\nComing soon:\n- Building a complete ETL pipeline\n- Scheduling with Airflow\n- Monitoring and logging\n- Testing pipelines\nEOF)",
      "Bash(docs/api/index.md << 'EOF'\n# API Reference\n\nComplete API reference for ForgeFlow.\n\n## Core Modules\n\n### Connectors\n\n- [BaseConnector]\\(connectors.md#baseconnector\\) - Abstract base class for connectors\n- [HTTPConnector]\\(connectors.md#httpconnector\\) - Generic HTTP connector\n- [RESTConnector]\\(connectors.md#restconnector\\) - RESTful API connector\n\n### Transformers\n\n- [BaseTransformer]\\(transformers.md#basetransformer\\) - Abstract base class\n- [JSONNormalizer]\\(transformers.md#jsonnormalizer\\) - JSON flattening transformer\n\n### Sinks\n\n- [BaseSink]\\(sinks.md#basesink\\) - Abstract base class for sinks\n- [PostgresSink]\\(sinks.md#postgressink\\) - PostgreSQL sink\n- [DuckDBSink]\\(sinks.md#duckdbsink\\) - DuckDB sink\n- [FileSink]\\(sinks.md#filesink\\) - File-based sink\n\n### Pipeline\n\n- [PipelineLoader]\\(pipeline.md#pipelineloader\\) - Load pipelines from YAML\n- [PipelineExecutor]\\(pipeline.md#pipelineexecutor\\) - Execute pipelines\n\n### Utilities\n\n- [RetryConfig]\\(utilities.md#retryconfig\\) - Retry configuration\n- [RateLimiter]\\(utilities.md#ratelimiter\\) - Rate limiting\n- [MemoryCache]\\(utilities.md#memorycache\\) - In-memory caching\n- [SchemaValidator]\\(utilities.md#schemavalidator\\) - Data validation\n\n## Usage Example\n\n```python\nimport asyncio\nfrom forgeflow.pipeline.loader import PipelineLoader\nfrom forgeflow.pipeline.executor import PipelineExecutor\n\nasync def main\\(\\):\n    # Load pipelines\n    pipelines = PipelineLoader.load_from_file\\('config/pipelines.yaml'\\)\n    \n    # Execute\n    executor = PipelineExecutor\\(\\)\n    for pipeline in pipelines:\n        if pipeline.get\\('enabled', True\\):\n            await executor.execute\\(pipeline\\)\n\nasyncio.run\\(main\\(\\)\\)\n```\n\n## CLI Reference\n\nSee the [CLI documentation]\\(cli.md\\) for command-line usage.\nEOF)"
    ]
  }
}
