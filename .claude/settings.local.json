{
  "permissions": {
    "allow": [
      "Bash(git add:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: configure pyproject.toml for pip distribution\n\n- Add complete package metadata \\(authors, keywords, classifiers\\)\n- Organize optional dependencies by feature \\(postgres, bigquery, s3, mongodb, snowflake\\)\n- Add airflow extra for Airflow integration\n- Add CLI entry point \\(dataforge command\\)\n- Move FastAPI/uvicorn to optional ''api'' extra\n- Add development tools \\(pytest-cov, pytest-mock, mypy, pre-commit\\)\n- Include project URLs \\(homepage, docs, repository, issues\\)\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add robust CLI with click and rich\n\nCommands implemented:\n- dataforge run <pipeline>: Execute pipelines with progress feedback\n- dataforge list: Display all pipelines in a formatted table\n- dataforge validate: Validate pipeline configuration\n- dataforge init <name>: Create new pipeline template\n- dataforge test <pipeline>: Test connector connections\n\nFeatures:\n- Rich console output with colors and tables\n- Progress bars with spinners\n- Error handling with detailed messages\n- Verbose mode for debugging\n- Configuration file support\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add Airflow integration with operators, hooks and sensors\n\nAirflow Components:\n- DataForgeOperator: Execute pipelines as Airflow tasks\n- DataForgeValidateOperator: Validate pipeline configs\n- DataForgeHook: Core hook for pipeline interaction\n- DataForgeSensor: Wait for pipeline config availability\n- DataForgeConnectionSensor: Wait for config file\n\nFeatures:\n- XCom support for passing results between tasks\n- Template fields for dynamic configuration\n- Validation before execution\n- Proper error handling and logging\n- UI colors for Airflow web interface\n\nExample DAGs:\n- Single pipeline execution with validation\n- Parallel pipeline execution\n- Sensor-based coordination\n\nInstall with: pip install data-forge[airflow]\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: add advanced ETL features \\(retry, rate limiting, caching, validation\\)\n\nRetry Logic:\n- RetryConfig for configurable retry behavior\n- Exponential backoff with tenacity integration\n- @with_retry decorator for async functions\n- CircuitBreaker pattern to prevent cascading failures\n\nRate Limiting:\n- RateLimiter: Token bucket algorithm\n- SlidingWindowRateLimiter: More accurate rate limiting\n- AdaptiveRateLimiter: Auto-adjusts based on response codes\n- Async context manager support\n\nCaching:\n- MemoryCache: In-memory LRU cache with TTL\n- DiskCache: Persistent disk-based cache\n- RequestCache: High-level HTTP request caching\n- Configurable backends \\(memory/disk\\)\n\nValidation:\n- SchemaValidator: Pydantic-based schema validation\n- DataQualityValidator: Custom quality rules \\(min/max, regex, enum\\)\n- Support for single items and batch validation\n\nAll modules include:\n- Structured logging with structlog\n- Async-first design\n- Comprehensive error handling\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "WebFetch(domain:alexwohlbruck.github.io)",
      "Bash(find:*)"
    ]
  }
}
